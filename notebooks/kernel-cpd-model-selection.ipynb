{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fixing the coding of reference.\n",
    "import os\n",
    "\n",
    "code_path = \"/Users/alejandrodelaconcha/Desktop/Doctorat/projects/Paper Change on Mean/research code\"  ### Change directory\n",
    "os.chdir(code_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import code generating the scenarios.\n",
    "from simulation_environment.scenarios_generator import *\n",
    "from models.variable_selection_detector import *\n",
    "from valuation_metrics.valuation_metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example using the first scenario described in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameters\n",
    "\n",
    "# Input.\n",
    "# 1) The number of change-points is generated via a Poisson distribution with mean \"mean_change-points\".\n",
    "# 2) The distance between change-points is at least 30 + an exponential distribution with mean \"mean_exponential\".\n",
    "# 3) An Erdos Graph with \"n_nodes\" and probability \"p_erdos\" is generated.\n",
    "# 4) The spectral profile of the filter is np.sqrt(15)/(np.log(x+10)+1).\n",
    "# 5) The lowest \"fixed_frequencies\" are generated at random previous to the first change-point.\n",
    "# 6) A given number \"random_frequencies\" are selected at random and they are modified after each change-point.\n",
    "\n",
    "n_nodes = 500\n",
    "fixed_frequencies = 100\n",
    "random_frequencies = 20\n",
    "mean_change_points = 5\n",
    "mean_exponential = 20\n",
    "p_erdos = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating the First Scenario.\n",
    "signal, GSO, change_points, mu, PSD, G = Scenario_I(\n",
    "    n_nodes,\n",
    "    mean_change_points,\n",
    "    fixed_frequencies,\n",
    "    random_frequencies,\n",
    "    mean_exponential,\n",
    "    p_erdos,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other datasets can be plugged. They only require to be numpy arrays of dimention Txp, where T is the number of Graph Signals and p is the number of nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel method logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of the algorithm is to obtain the number the change point detection problem by solving the penalized problem:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{d}=\\operatorname{argmin}_{ d \\in \\{1,...,D_{max} \\} } C(y,d) + pen(d), \n",
    "\\end{align}\n",
    "\n",
    "where $C(y,d)$ is the value of the cost function evaluated at the best partition of lenght $d$ and $pen(d)$ is a partition for the number of change-points:\n",
    "\n",
    "\\begin{align}\n",
    "pen(d)= \\frac{d}{N}(c_1 + c_2 {{N-1}\\choose{d-1}} )\n",
    "\\end{align}\n",
    "\n",
    "The paper A Kernel Multiple Change-point Algorithm via Model Selection Arlot aims to recover the constants $c_1$ and $c_2$. \n",
    "\n",
    "\n",
    "This is done using a linear regression of $C(y,d)$ against the terms $\\frac{d}{N}$ and $\\frac{d}{N}{{N-1}\\choose{d-1}}$ and multipling the results by -2. \n",
    "\n",
    "Here I put explicetly the code so can see directly how it work in practice and think about how can we added to the library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruptures as rpt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import scipy\n",
    "\n",
    "\n",
    "class Kernel_model_selection:\n",
    "    def __init__(self, D_max, cost_fuction, coefs=None):\n",
    "\n",
    "        ##### Function initializing the class\n",
    "\n",
    "        ### Input\n",
    "\n",
    "        ## D_max = maximum number of change-points to chacke\n",
    "        ## cost_function = cost function object from the ruptures library\n",
    "        ## coefs: list with the coeficients for the cost_function ralated with the number of change-points/\n",
    "\n",
    "        self.D_max = D_max\n",
    "        self.cost_function = cost_fuction\n",
    "        self.coefs = coefs\n",
    "\n",
    "    def fit(self, data):\n",
    "\n",
    "        #### Function which finds the diferent partitions from size 0 up to size D_max\n",
    "\n",
    "        ### Input\n",
    "        ## data= matrix of size Txp where T is the time horizon, p the number of nodes.\n",
    "\n",
    "        self.change_points = []\n",
    "        self.data = data\n",
    "        self.T = data.shape[0]\n",
    "\n",
    "        self.change_points.append([self.T])\n",
    "\n",
    "        ## Solving the change-point detection problem usig Dynamic Programming.\n",
    "\n",
    "        detector = rpt.Dynp(custom_cost=self.cost_function, min_size=2, jump=2).fit(\n",
    "            data\n",
    "        )\n",
    "\n",
    "        for d in range(1, self.D_max):\n",
    "            self.change_points.append(detector.predict(n_bkps=d))\n",
    "\n",
    "    def predict(self):\n",
    "        ### Function selecting the optimal number of change-points via a model selection approach.\n",
    "\n",
    "        ### Output\n",
    "        ## optimal partition.\n",
    "\n",
    "        self.get_partitions_cost()\n",
    "        if self.coefs is None:\n",
    "            self.get_constants()\n",
    "        log_comb = np.array(\n",
    "            [self.log_comb(self.T - 1, d) for d in np.arange(0, self.D_max)]\n",
    "        )\n",
    "        index = np.argmin(\n",
    "            self.partition_cost\n",
    "            + (1.0 / self.T)\n",
    "            * (self.coefs[0] * log_comb + self.coefs[1] * np.arange(1, self.D_max + 1))\n",
    "        )\n",
    "\n",
    "        self.predicted_change_points = self.change_points[index]\n",
    "\n",
    "        return self.predicted_change_points\n",
    "\n",
    "    def get_partitions_cost(self):\n",
    "        #### Function estimating the total cost for a given partition.\n",
    "\n",
    "        self.partition_cost = np.zeros(self.D_max)\n",
    "\n",
    "        self.partition_cost[0] = self.cost_function.error(0, self.change_points[0][0])\n",
    "        for d in range(1, len(self.change_points)):\n",
    "\n",
    "            self.partition_cost[d] = self.cost_function.error(\n",
    "                0, self.change_points[d][0]\n",
    "            )\n",
    "\n",
    "            for i in range(1, len(self.change_points[d])):\n",
    "\n",
    "                self.partition_cost[d] += self.cost_function.error(\n",
    "                    self.change_points[d][i - 1], self.change_points[d][i]\n",
    "                )\n",
    "\n",
    "    def get_constants(self, lower_bound=None):\n",
    "\n",
    "        #### Function computing the constants which are required for the computation of the\n",
    "        ## optimal number of change-points (This does the linear regression )\n",
    "\n",
    "        ### Input\n",
    "        ## lower_bound: left side of the interval where the regression is done.\n",
    "\n",
    "        if lower_bound is None:\n",
    "            lower_bound = np.floor(0.6 * self.D_max)\n",
    "\n",
    "        rank_D = np.arange(lower_bound, self.D_max + 1)\n",
    "        y = self.partition_cost[[int(x) - 1 for x in rank_D]]\n",
    "        log_comb = np.array([self.log_comb(self.T - 1, d) for d in (rank_D - 1)])\n",
    "        x = np.array((log_comb / self.T, rank_D / self.T)).transpose()\n",
    "        ### The coefficients are obtained via robust linear regression.\n",
    "        self.coefs = LinearRegression().fit(x, y).coef_\n",
    "        self.coefs = -2.0 * self.coefs\n",
    "\n",
    "    def log_comb(self, T, d):\n",
    "        ### This function estimates the log combinations of T in n to avoid numerical errors.\n",
    "        return np.sum(np.log(np.arange(d + 1, T + 1))) - np.sum(\n",
    "            np.log(np.arange(1, T - d + 1))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change-point detection using Kernel Methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Fix the right parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = signal.shape[0]\n",
    "D_max = int(T / np.log(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_kernel_cost = rpt.costs.CostMl(metric=np.eye(n_nodes)).fit(\n",
    "    signal\n",
    ")  ## Linear Kernel\n",
    "laplacian_kernel_cost = rpt.costs.CostMl(metric=GSO).fit(signal)  ## Laplacian Kernel\n",
    "gaussian_kernel_cost = rpt.costs.CostRbf().fit(signal)  ## Gaussian Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Train each of the models and predict the change-points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Linear Kernel\n",
    "linear_kernel_detector = Kernel_model_selection(D_max, linear_kernel_cost)\n",
    "linear_kernel_detector.fit(signal)\n",
    "change_points_linear = linear_kernel_detector.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Laplacian Kernel\n",
    "laplacian_kernel_detector = Kernel_model_selection(D_max, laplacian_kernel_cost)\n",
    "laplacian_kernel_detector.fit(signal)\n",
    "change_points_laplacian = laplacian_kernel_detector.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gaussian Kernel\n",
    "Gaussian_kernel_detector = Kernel_model_selection(D_max, gaussian_kernel_cost)\n",
    "Gaussian_kernel_detector.fit(signal)\n",
    "change_points_gaussian = Gaussian_kernel_detector.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Evaluate the performance of each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    haussdorf_distance_linear,\n",
    "    randindex_linear,\n",
    "    precision_linear,\n",
    "    recall_linear,\n",
    "    F1_linear,\n",
    ") = get_valuation_metrics(change_points, change_points_linear)\n",
    "print(\"     Linear Kernel     \")\n",
    "print(\"real_change_points: \" + str(change_points))\n",
    "print(\"estimated_change_points: \" + str(change_points_linear))\n",
    "print(\"Haussdor distance: \" + str(haussdorf_distance_linear))\n",
    "print(\"Randindex: \" + str(randindex_linear))\n",
    "print(\"Precision: \" + str(precision_linear))\n",
    "print(\"Recall: \" + str(recall_linear))\n",
    "print(\"F1: \" + str(F1_linear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    haussdorf_distance_laplacian,\n",
    "    randindex_laplacian,\n",
    "    precision_laplacian,\n",
    "    recall_laplacian,\n",
    "    F1_laplacian,\n",
    ") = get_valuation_metrics(change_points, change_points_laplacian)\n",
    "print(\"     Laplacian Kernel     \")\n",
    "print(\"real_change_points: \" + str(change_points))\n",
    "print(\"estimated_change_points: \" + str(change_points_laplacian))\n",
    "print(\"Haussdor distance: \" + str(haussdorf_distance_laplacian))\n",
    "print(\"Randindex: \" + str(randindex_laplacian))\n",
    "print(\"Precision: \" + str(precision_laplacian))\n",
    "print(\"Recall: \" + str(recall_laplacian))\n",
    "print(\"F1: \" + str(F1_laplacian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    haussdorf_distance_gaussian,\n",
    "    randindex_gaussian,\n",
    "    precision_gaussian,\n",
    "    recall_gaussian,\n",
    "    F1_gaussian,\n",
    ") = get_valuation_metrics(change_points, change_points_gaussian)\n",
    "print(\"     Gaussian Kernel     \")\n",
    "print(\"real_change_points: \" + str(change_points))\n",
    "print(\"estimated_change_points: \" + str(change_points_gaussian))\n",
    "print(\"Haussdor distance: \" + str(haussdorf_distance_gaussian))\n",
    "print(\"Randindex: \" + str(randindex_gaussian))\n",
    "print(\"Precision: \" + str(precision_gaussian))\n",
    "print(\"Recall: \" + str(recall_gaussian))\n",
    "print(\"F1: \" + str(F1_gaussian))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
